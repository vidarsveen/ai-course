<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Concepts Explorer - Context Models</title>
    <!-- Load Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Load Inter font -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">

    <style>
        /* Custom styles */
        body {
            font-family: 'Inter', sans-serif;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        /* Original token styles (kept for consistency) */
        .token-box {
            background-color: #374151; /* gray-700 */
            border: 1px solid #4f46e5; /* indigo-600 */
            color: #e0e7ff; /* indigo-100 */
            padding: 4px 10px;
            border-radius: 6px;
            font-family: 'monospace';
            font-weight: 500;
            margin: 2px;
            display: inline-block;
            transition: all 0.2s;
        }

        /* === STYLES FOR N-GRAM vs ATTENTION === */

        .ngram-window {
            background-color: #4b5563; /* gray-600 */
            border: 1px solid #9ca3af; /* gray-400 */
            color: #e5e7eb; /* gray-200 */
            padding: 2px 6px;
            border-radius: 4px;
        }

        .attention-focus {
            background-color: #991b1b; /* red-800 */
            border: 1px solid #f87171; /* red-400 */
            color: #fee2e2; /* red-100 */
            padding: 4px 10px;
            border-radius: 6px;
            font-family: 'monospace';
            font-weight: 500;
            margin: 2px;
            display: inline-block;
            transition: all 0.2s;
            box-shadow: 0 0 12px rgba(248, 113, 113, 0.5);
        }

        .attention-target {
            background-color: #1d4ed8; /* blue-700 */
            border: 1px solid #60a5fa; /* blue-400 */
            color: #dbeafe; /* blue-100 */
            padding: 4px 10px;
            border-radius: 6px;
            font-family: 'monospace';
            font-weight: 500;
            margin: 2px;
            display: inline-block;
            transition: all 0.2s;
        }

        .attention-focus-text {
            color: #f87171; /* red-400 */
            font-weight: 600;
            font-family: 'monospace';
        }
        .attention-target-text {
            color: #60a5fa; /* blue-400 */
            font-weight: 600;
            font-family: 'monospace';
        }

    </style>
</head>
<body class="bg-gray-900 text-gray-200 font-inter p-4 md:p-8 min-h-screen">

    <div class="max-w-3xl mx-auto"> <!-- Centered single column -->
        <header class="text-center mb-8">
            <h1 class="text-3xl md:text-4xl font-bold text-white">Interactive LLM Concepts</h1>
            <p class="text-lg text-gray-400 mt-2">N-Grams vs. Transformers</p>
        </header>

        <div class="grid grid-cols-1 gap-8">

            <!-- === N-GRAM vs ATTENTION CARD === -->
            <div class="bg-gray-800 p-6 rounded-lg shadow-2xl border border-gray-700 flex flex-col">
                <h2 class="text-2xl font-bold text-white mb-4">How Models Understand Context</h2>
                <p class="text-gray-300 leading-relaxed mb-6">
                    Older models and modern models understand context very differently.
                </p>

                <!-- N-Gram Section -->
                <div class="mb-6">
                    <h3 class="text-xl font-bold text-white mb-3">1. N-Grams (Fixed Window)</h3>
                    <p class="text-gray-300 my-2">
                        <strong>N-Grams</strong> look at a fixed window of "N" words to understand context (e.g., 2 words, 3 words). They only see immediate neighbors and have a very short-term memory.
                    </p>
                    <div class="p-4 bg-gray-900 rounded-md">
                        <p class="font-mono text-gray-300 my-2 text-lg">"...The <span class="ngram-window">quick brown fox</span> jumped..."</p>
                        <p class="text-gray-400 mt-3">
                            To predict the next word after "quick brown", a 3-gram model *only* sees "quick brown". It has no idea what came before "The". This fails for long-range context (e.g., "The cat... was tired.").
                        </p>
                    </div>
                </div>

                <!-- Transformer/Attention Section -->
                <div>
                    <h3 class="text-xl font-bold text-white mb-3">2. Transformers (Attention Mechanism)</h3>
                    <p class="text-gray-300 my-2">
                        <strong>Transformers</strong> use an <strong>Attention</strong> mechanism. This allows them to look at the *entire* sentence at once and decide which *other* words are most important for understanding each specific word, no matter how far apart they are.
                    </p>
                    <div class="p-4 bg-gray-900 rounded-md">
                        <div class="font-mono text-gray-300 my-2 text-lg leading-loose">
                            <span class="token-box attention-target">The</span>
                            <span class="token-box attention-target">cat</span>
                            <span class="token-box">sat</span>
                            <span class="token-box">on</span>
                            <span class="token-box">the</span>
                            <span class="token-box">mat</span>
                            <span class="token-box">because</span>
                            <span class="token-box attention-focus">it</span>
                            <span class="token-box">was</span>
                            <span class="token-box">tired.</span>
                        </div>
                        <p class="text-gray-400 mt-3">
                            When the model processes the word <span class="attention-focus-text">"it"</span>, the attention mechanism creates a strong connection (or "pays attention") back to <span class="attention-target-text">"The cat"</span>, correctly understanding what "it" refers to.
                        </p>
                    </div>
                </div>

                <!-- Keywords -->
                <div class="mt-auto pt-6 border-t border-gray-700 mt-8">
                    <h3 class="text-sm font-semibold text-gray-400 uppercase tracking-wider mb-3">Keywords</h3>
                    <div class="flex flex-wrap gap-2">
                        <span class="text-xs bg-gray-700 text-blue-300 px-2 py-0.5 rounded-full font-mono">n-gram</span>
                        <span class="text-xs bg-gray-700 text-blue-300 px-2 py-0.5 rounded-full font-mono">transformer</span>
                        <span class="text-xs bg-gray-700 text-blue-300 px-2 py-0.foo5 rounded-full font-mono">attention mechanism</span>
                        <span class="text-xs bg-gray-700 text-blue-300 px-2 py-0.5 rounded-full font-mono">context window</span>
                        <span class="text-xs bg-gray-700 text-blue-300 px-2 py-0.5 rounded-full font-mono">long-range dependency</span>
                    </div>
                </div>
            </div>

        </div>
    </div>

    <!-- No script tag is needed as this is a static info page -->

</body>
</html>
